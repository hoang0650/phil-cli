version: '3.9'

services:
  # --- 1. SECURITY GATEWAY (NGINX) ---
  gateway:
    image: nginx:latest
    container_name: phil_gateway
    restart: always
    ports:
      - "80:80" # Public Port
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/.htpasswd:/etc/nginx/.htpasswd:ro
    depends_on:
      - api-server
    networks:
      - phil_network

  # --- 2. AI BRAINS (MODELS) ---
  llm-coder: # Llama-3-70B
    image: vllm/vllm-openai:latest
    container_name: ai_brain_coder
    runtime: nvidia
    restart: always
    environment: [ "CUDA_VISIBLE_DEVICES=0" ]
    command: >
      --model casperhansen/llama-3-70b-instruct-awq --quantization awq
      --dtype half --max-model-len 8192 --gpu-memory-utilization 0.50
      --port 8000 --host 0.0.0.0
    volumes: [ "./models:/root/.cache/huggingface" ]
    networks: [ "phil_network" ]

  llm-vietnamese: # PhoGPT
    image: vllm/vllm-openai:latest
    container_name: ai_brain_vietnamese
    runtime: nvidia
    restart: always
    environment: [ "CUDA_VISIBLE_DEVICES=0" ]
    command: >
      --model vinai/PhoGPT-4B-Chat --trust-remote-code
      --dtype half --gpu-memory-utilization 0.10
      --port 8001 --host 0.0.0.0
    volumes: [ "./models:/root/.cache/huggingface" ]
    networks: [ "phil_network" ]

  llm-vision: # Qwen2-VL
    image: vllm/vllm-openai:latest
    container_name: ai_brain_eyes
    runtime: nvidia
    restart: always
    environment: [ "CUDA_VISIBLE_DEVICES=0" ]
    command: >
      --model Qwen/Qwen2-VL-7B-Instruct-AWQ --quantization awq
      --dtype half --gpu-memory-utilization 0.10 --max-model-len 4096
      --limit-mm-per-prompt image=1 --port 8002 --host 0.0.0.0
    volumes: [ "./models:/root/.cache/huggingface" ]
    networks: [ "phil_network" ]

  stt-engine: # Whisper
    image: fedirz/faster-whisper-server:latest-cuda
    container_name: ai_brain_ears
    runtime: nvidia
    restart: always
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - WHISPER_MODEL=large-v3
    volumes: [ "./models:/root/.cache/huggingface" ]
    networks: [ "phil_network" ]

  tts-engine: # XTTS
    image: ghcr.io/coqui-ai/tts:latest
    container_name: ai_brain_mouth
    runtime: nvidia
    restart: always
    environment: [ "CUDA_VISIBLE_DEVICES=0" ]
    command: "python3 -m TTS.server.server --model_name tts_models/multilingual/multi-dataset/xtts_v2 --port 8004 --use_cuda true"
    volumes: [ "./models:/root/.local/share/tts" ]
    networks: [ "phil_network" ]

  # --- 3. LOGIC & EXECUTION ---
  
  # Sandbox: Nơi code thực thi (Isolated)
  code-sandbox:
    build: 
      context: .
      dockerfile: sandbox/Dockerfile
    container_name: ai_sandbox
    command: tail -f /dev/null
    volumes:
      - ./workspace:/workspace                 # Shared storage
      - ./skills:/workspace/skills             # Skill library
      - ./mcp_servers_config.json:/workspace/mcp_servers_config.json # MCP Config
    working_dir: /workspace
    networks: [ "phil_network" ]

  # API Server: Controller trung tâm (FastAPI)
  api-server:
    build: 
      context: .
      dockerfile: sandbox/Dockerfile
    container_name: ai_api_server
    command: python3 src/api_server.py
    volumes:
      - .:/app
      - ./workspace:/workspace
      - ./skills:/app/skills
      - /var/run/docker.sock:/var/run/docker.sock # Cho phép điều khiển Sandbox
    env_file: [ ".env" ]
    depends_on: [ "llm-coder", "code-sandbox" ]
    networks: [ "phil_network" ]

networks:
  phil_network:
    driver: bridge